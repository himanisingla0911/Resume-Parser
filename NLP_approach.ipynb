{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file is used to extract the skills from  Resume by comparing the keywords with our pre-defined ist of skills "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pymupdf python-docx pytesseract Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importing the neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import io\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Provide the full path to the Tesseract executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r'C:/Program Files/Tesseract-OCR/tesseract.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Resume file(pdf, word, pdf with images) from a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path):\n",
    "    text = \"\"\n",
    "    doc = fitz.open(file_path)\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "        \n",
    "        # Check for images in the page\n",
    "        image_list = page.get_images(full=True)\n",
    "        for img_index, img in enumerate(image_list, start=1):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            # Use full path to Tesseract executable and specify language\n",
    "            text += pytesseract.image_to_string(image, lang='eng')\n",
    "    \n",
    "    return text\n",
    "\n",
    "def read_word(file_path):\n",
    "    doc = Document(file_path)\n",
    "    text = []\n",
    "    for para in doc.paragraphs:\n",
    "        text.append(para.text)\n",
    "    return \"/n\".join(text)\n",
    "\n",
    "def read_files_from_folder(folder_path):\n",
    "    all_text = \"\"\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            if file_name.lower().endswith('.pdf'):\n",
    "                all_text += read_pdf(file_path) + \"/n\"\n",
    "            elif file_name.lower().endswith('.docx'):\n",
    "                all_text += read_word(file_path) + \"/n\"\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reading Resume content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLAKE THORNBROCK\n",
      "BUSINESS ANALYST\n",
      "CONTACT\n",
      "blake.thorn@email.com\n",
      "(123) 456-7890\n",
      "Boston, MA\n",
      "LinkedIn\n",
      "EDUCATION\n",
      "B.S.\n",
      "Business\n",
      "Boston University\n",
      "September 2010 - April 2014\n",
      "Boston, MA\n",
      "SKILLS\n",
      "SQL\n",
      "Excel/ Google Sheets\n",
      "Tableau\n",
      "Google Analytics\n",
      "Leadership Experience\n",
      "Problem Solving\n",
      "WORK EXPERIENCE\n",
      "Business Analyst\n",
      "Lancer Insurance Company\n",
      "May 2018 - current / Boston, MA\n",
      "· Increased revenue by $16M through analysis of data\n",
      "surrounding customer behaviors, vendor relationships,\n",
      "stakeholder goals, and workflows\n",
      "· Discovered an additional $3M in savings after compiling data\n",
      "and identifying dead industry trends\n",
      "· Researched and prepared presentations for C-level suite\n",
      "regarding profitability reports, policies, internal operating\n",
      "inefficiencies, and industry trends\n",
      "· Participated in product development planning, providing key\n",
      "insights based on predictive modeling, which allowed PMs to\n",
      "circumvent a $50K legal fine\n",
      "Business Analyst\n",
      "ThreeBridge Solutions\n",
      "August 2016 - May 2018 / Boston, MA\n",
      "· Traveled 3 of 5 days to work with clients, observing business\n",
      "processes, interviewing staff, and documenting practices\n",
      "· Conducted research, analyzed business operations, and\n",
      "problem solved operating inefficiencies, saving companies a\n",
      "combined 800 man hours a month\n",
      "· Worked with stakeholders and clients to model and document\n",
      "goals, communicating transparently to meet 98% of deadlines\n",
      "· Modeled data to generate reports comparing business process\n",
      "evolution, assisting C-level staff with appropriate KPIs based on\n",
      "improved operations\n",
      "Product Modeling Analyst\n",
      "Geico\n",
      "August 2014 - August 2016 / Boston, MA\n",
      "· Identified operating improvements from internal data with\n",
      "SQL, which reduced man hours by 8% year over year\n",
      "· Tracked, extrapolated, and interpreted customer data using\n",
      "Python, SQL, and Excel to report customer behaviors and state-\n",
      "wide retention numbers\n",
      "· Presented predictive modeling insights to C-level suite and\n",
      "stakeholders, participating in decisions surrounding policy\n",
      "packages that saved company $3.2M in legal costs in 2015\n",
      "· Worked with 3 analysts to institute a new modeling technique\n",
      "that saved 60 quarterly hours in customer comparisons\n",
      "/n\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"data\"\n",
    "all_text = read_files_from_folder(folder_path)\n",
    "print(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLAKE THORNBROCK BUSINESS ANALYST CONTACT blake.thorn@email.com (123) 456-7890 Boston, MA LinkedIn EDUCATION B.S. Business Boston University September 2010 - April 2014 Boston, MA SKILLS SQL Excel/ Google Sheets Tableau Google Analytics Leadership Experience Problem Solving WORK EXPERIENCE Business Analyst Lancer Insurance Company May 2018 - current / Boston, MA · Increased revenue by $16M through analysis of data surrounding customer behaviors, vendor relationships, stakeholder goals, and workflows · Discovered an additional $3M in savings after compiling data and identifying dead industry trends · Researched and prepared presentations for C-level suite regarding profitability reports, policies, internal operating inefficiencies, and industry trends · Participated in product development planning, providing key insights based on predictive modeling, which allowed PMs to circumvent a $50K legal fine Business Analyst ThreeBridge Solutions August 2016 - May 2018 / Boston, MA · Traveled 3 of 5 days to work with clients, observing business processes, interviewing staff, and documenting practices · Conducted research, analyzed business operations, and problem solved operating inefficiencies, saving companies a combined 800 man hours a month · Worked with stakeholders and clients to model and document goals, communicating transparently to meet 98% of deadlines · Modeled data to generate reports comparing business process evolution, assisting C-level staff with appropriate KPIs based on improved operations Product Modeling Analyst Geico August 2014 - August 2016 / Boston, MA · Identified operating improvements from internal data with SQL, which reduced man hours by 8% year over year · Tracked, extrapolated, and interpreted customer data using Python, SQL, and Excel to report customer behaviors and state- wide retention numbers · Presented predictive modeling insights to C-level suite and stakeholders, participating in decisions surrounding policy packages that saved company $3.2M in legal costs in 2015 · Worked with 3 analysts to institute a new modeling technique that saved 60 quarterly hours in customer comparisons /n\n"
     ]
    }
   ],
   "source": [
    "stripped_all_text = ' '.join(all_text.split())\n",
    "print(stripped_all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read text file containing dataset of skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['# Programming and Technical Skills:      Python', 'R', 'SQL', 'Java', 'Scala', 'C', 'C++', 'SAS', 'JavaScript', 'TypeScript', 'Go', 'Ruby', 'PHP', 'Swift', 'Kotlin', 'Rust', 'Perl', 'MATLAB', '# Data Handling and Processing  Data Wrangling', 'ETL (Extract', 'Transform', 'Load)', 'Hadoop', 'Spark', 'Kafka', 'Data Storage (SQL', 'NoSQL)', 'Data Warehousing (Redshift', 'Snowflake)', 'Data Lakes', 'Big Data', 'Data Mining', 'Airflow', '# Statistical and Mathematical Skills  Statistics', 'Linear Algebra', 'Calculus', 'Probability Theory', 'Discrete Mathematics', 'Optimization', '# Machine Learning and AI  Supervised Learning', 'Unsupervised Learning', 'Deep Learning', 'Natural Language Processing (NLP)', 'Computer Vision', 'Model Evaluation (Cross-validation', 'A/B testing', 'ROC curves)', 'Reinforcement Learning', 'Feature Engineering', 'Time Series Analysis', 'Generative Adversarial Networks (GANs)', 'Recommendation Systems', '# Data Visualization  Tableau', 'Power BI', 'Matplotlib', 'Seaborn', 'D3.js', 'ggplot2', 'Plotly', 'Dash', '# Software Development  Agile', 'Scrum', 'Kanban', 'DevOps', 'CI/CD', 'Docker', 'Kubernetes', 'Microservices', 'REST APIs', 'GraphQL', 'Unit Testing', 'Integration Testing', 'TDD (Test-Driven Development)', 'BDD (Behavior-Driven Development)', 'Continuous Integration', '# Web Development  HTML', 'CSS', 'React', 'Angular', 'Vue.js', 'Node.js', 'Django', 'Flask', 'Ruby on Rails', 'Spring Boot', '# Mobile Development  iOS Development', 'Android Development', 'React Native', 'Flutter', 'Xamarin', '# Cloud Computing  AWS (Amazon Web Services)', 'Azure', 'Google Cloud Platform', 'CloudFormation', 'Terraform', 'Serverless', '# Database Management  MySQL', 'PostgreSQL', 'MongoDB', 'Redis', 'Cassandra', 'Oracle Database', 'SQL Server', 'Elasticsearch', '# Networking  TCP/IP', 'DNS', 'Load Balancing', 'Firewalls', 'VPN', 'Network Security', '# Cybersecurity  Ethical Hacking', 'Penetration Testing', 'Cryptography', 'Incident Response', 'Threat Modeling', 'SIEM (Security Information and Event Management)', '# Soft Skills  Communication', 'Problem-Solving', 'Collaboration', 'Critical Thinking', 'Project Management', 'Leadership', 'Time Management', 'Adaptability', 'Conflict Resolution', 'Negotiation', '# Management and Business Skills  Product Management', 'Business Analysis', 'Stakeholder Management', 'Risk Management', 'Financial Analysis', 'Marketing Strategy', 'Salesforce', 'Customer Relationship Management (CRM)', '# Other Essential Skills  Data Ethics', 'Domain Knowledge', 'Cloud Computing (AWS', 'Azure', 'Google Cloud Platform)', 'Version Control (Git)', 'APIs', 'Blockchain', 'IoT (Internet of Things)', 'Edge Computing', 'Quantum Computing', 'Augmented Reality (AR)', 'Virtual Reality (VR)', '3D Printing', 'Robotics']\n"
     ]
    }
   ],
   "source": [
    "with open(\"skills_dataset.txt\", \"r\") as my_file:\n",
    "    data = my_file.read()\n",
    "    data_into_list = data.replace(\"\\n\", \" \").split(\",\")\n",
    "    new_list = [word.strip() for word in data_into_list]\n",
    "    print(new_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To extract the skills from resume by comapring it with our skill.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['R', 'SQL', 'C', 'Go', 'Leadership']\n"
     ]
    }
   ],
   "source": [
    "resume_skills = [word for word in new_list if word in stripped_all_text]\n",
    "print(resume_skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To extract the Email:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blake.thorn@email.com\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def extract_email(resume_text):\n",
    "    email_regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    match = re.search(email_regex, resume_text)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return None\n",
    "\n",
    "email = extract_email(stripped_all_text)\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Extract the linekedin URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def extract_linkedin_url(resume_text):\n",
    "    linkedin_regex = r'(https?://(?:www\\.)?linkedin\\.com/in/\\S+)'\n",
    "    match = re.search(linkedin_regex, resume_text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "linkedin_url = extract_linkedin_url(stripped_all_text)\n",
    "print(linkedin_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To extract the Mobile Number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123)456-7890\n"
     ]
    }
   ],
   "source": [
    "def extract_phone_number(resume_text):\n",
    "    phone_regex1 = r'\\+\\d+\\s*-\\s*\\d+\\s*-\\s*\\d+\\s*-\\s*\\d+'\n",
    "    phone_regex2 = r'\\(\\d{3}\\)\\s*\\d{3}-\\d{4}'\n",
    "    match1 = re.search(phone_regex1, resume_text)\n",
    "    match2 = re.search(phone_regex2, resume_text)\n",
    "    if match1:\n",
    "        # Replace hyphens and whitespace with spaces\n",
    "        formatted_number = match1.group().replace('\\n', '')\n",
    "        return formatted_number\n",
    "    elif match2:\n",
    "        # Replace parentheses and whitespace with spaces\n",
    "        formatted_number = match2.group().replace(' ', '')\n",
    "        return formatted_number\n",
    "    return None\n",
    "phn = extract_phone_number(stripped_all_text)\n",
    "print(phn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To extract the URL links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://linkedin.com/in/blake-thornbrock']\n"
     ]
    }
   ],
   "source": [
    "import pdfx\n",
    "def get_urls_from_pdf_in_folder(folder_path):\n",
    "    '''extract urls from the first pdf file found in the folder'''\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.lower().endswith('.pdf'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            pdf = pdfx.PDFx(file_path)\n",
    "\n",
    "            # get urls\n",
    "            pdf_url_dict = pdf.get_references_as_dict()\n",
    "\n",
    "            if \"url\" in pdf_url_dict.keys():\n",
    "                return pdf_url_dict[\"url\"]\n",
    "    \n",
    "    return []\n",
    "\n",
    "folder_path = \"data\"  # give the path of the folder in which the file is present\n",
    "# Get URLs from the first PDF file found in the folder\n",
    "urls = get_urls_from_pdf_in_folder(folder_path)\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store all outputs in a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "data = {\n",
    "    \"skills\": resume_skills,\n",
    "    \"email\": email,\n",
    "    \"linkedin_url\": linkedin_url,\n",
    "    \"phone_number\": phn,\n",
    "    \"urls\": urls\n",
    "}\n",
    "\n",
    "with open(\"links_skills.json\", \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOTHER METHOD\n",
    "\n",
    "- creating a list of skills containing all the skills keywords \n",
    "- and comparing it with the Uploaded resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_science_skills = [\n",
    "    # Programming and Technical Skills\n",
    "    \"Python\",\n",
    "    \"R\",\n",
    "    \"SQL\",\n",
    "    \"Java\",\n",
    "    \"Scala\",\n",
    "    \"C\",\n",
    "    \"C++\",\n",
    "    \"SAS\",\n",
    "    \n",
    "    # Data Handling and Processing\n",
    "    \"Data Wrangling\",\n",
    "    \"ETL (Extract, Transform, Load)\",\n",
    "    \"Hadoop\",\n",
    "    \"Spark\",\n",
    "    \"Kafka\",\n",
    "    \"Data Storage (SQL, NoSQL)\",\n",
    "    \"Data Warehousing (Redshift, Snowflake)\",\n",
    "    \n",
    "    # Statistical and Mathematical Skills\n",
    "    \"Statistics\",\n",
    "    \"Linear Algebra\",\n",
    "    \"Calculus\",\n",
    "    \"Probability Theory\",\n",
    "    \n",
    "    # Machine Learning and AI\n",
    "    \"Supervised Learning\",\n",
    "    \"Unsupervised Learning\",\n",
    "    \"Deep Learning\",\n",
    "    \"Natural Language Processing (NLP)\",\n",
    "    \"Computer Vision\",\n",
    "    \"Model Evaluation (Cross-validation, A/B testing, ROC curves)\",\n",
    "    \n",
    "    # Data Visualization\n",
    "    \"Tableau\",\n",
    "    \"Power BI\",\n",
    "    \"Matplotlib\",\n",
    "    \"Seaborn\",\n",
    "    \"D3.js\",\n",
    "    \n",
    "    # Soft Skills\n",
    "    \"Communication\",\n",
    "    \"Problem-Solving\",\n",
    "    \"Collaboration\",\n",
    "    \"Critical Thinking\",\n",
    "    \"Project Management\",\n",
    "    \n",
    "    # Other Essential Skills\n",
    "    \"Data Ethics\",\n",
    "    \"Domain Knowledge\",\n",
    "    \"Cloud Computing (AWS, Azure, Google Cloud Platform)\",\n",
    "    \"Version Control (Git)\",\n",
    "    \"APIs\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'R', 'SQL', 'C', 'Tableau']\n"
     ]
    }
   ],
   "source": [
    "all_text_lower = stripped_all_text.lower()\n",
    "\n",
    "# Filter skills that are present in the text\n",
    "matched_skills = [skill for skill in data_science_skills if skill.lower() in all_text_lower]\n",
    "\n",
    "# Print the matched skills\n",
    "print(matched_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
